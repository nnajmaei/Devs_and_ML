{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b433b2d9-c43d-4659-b30b-01bb319ed481",
   "metadata": {},
   "source": [
    "# Practical Linear Algebra for Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289c203-f77c-45c5-abc8-89a75cd0e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup animation\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "\n",
    "# to read an image from a url (io) and convert it to grayscale (color)\n",
    "from skimage import io,color\n",
    "# convolution\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.linalg import hilbert\n",
    "\n",
    "import scipy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import sympy as sym\n",
    "import time\n",
    "\n",
    "\n",
    "# NOTE: these lines define global figure properties used for publication.\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg') # display figures in vector format\n",
    "plt.rcParams.update({'font.size':14}) # set global font size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f3812-ff0b-4803-85b0-d38535840218",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### ============================\n",
    "# Chapter 11\n",
    "## General Linear Models and Least Squares\n",
    "\n",
    "\n",
    "### GLM\n",
    "\n",
    "\n",
    "$$X\\beta=y \\implies \\beta=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "if $X$ is the matrix of known indepent parameters, and $y$ is the vector of known dependents, then finding $\\beta$ will give us a model of the relationship between $X$ and $y$. This is called the <u><i><b>least square solution</b></i></u>.\n",
    "\n",
    "Neat tip: If $XB=Y$ and $Y=I$, then by solving this equation $B$ will be the inverse of Y.\n",
    "\n",
    "<u><i><b>Is the solution exact?</b></i></u> The solution is only exact if $y$ is in the column space of $X$.\n",
    "So we allow a discripency $\\epsilon$ in the result $X\\beta=y+\\epsilon$\n",
    "\n",
    "<u><i><b>Geometric Intrerp:</b></i></u> Our goal is to find a set of coefficients $\\beta$ such that the weighter lin combination of the columns of the vectors (the column space of vector $\\matrix{C}(X)$ is the closest point to $y$, and the difference or projection vector is vector $\\epsilon$. So $X^T\\epsilon$ should be zero becuase they are ortogonal and from there $\\implies \\beta=(X^TX)^{-1}X^Ty$.\n",
    "\n",
    "- We also can define it as a minimization porblem: $\\min_{\\beta}||\\epsilon||^2=\\min_{\\beta}||X\\beta-y||^2$\n",
    "\n",
    "- If we have a constant (e.g., $mx+b=y$), then we can add the constant to the $X$ as a column of 1, an to $\\beta$ as a new regressor in a row.\n",
    "\n",
    "### LS via QR\n",
    "To increase numerical stability and avoiding calculating inverse of X:\n",
    "\n",
    "$$X\\beta=y \\implies QR\\beta=y \\implies \\beta=R^{-1}Q^Ty$$\n",
    "\n",
    "We can also solve this using RREF, by forming the equation as $R\\beta=Q^Ty$. Then we use the augmented matrix $[R|Q^Ty]$ to calculate the RREF. NOTE that if X is tall, then use the \"reduced\" version of RREF function, so R hast the same number of rows as the uknown variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311eb23c-abf5-41a5-a3b5-a13c7d5b9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining predictors or independent vars\n",
    "X=np.array([[1,70,177],[1,86,190],[1,63,180],[1,62,172]])\n",
    "# defining observations or dependent vars\n",
    "y=np.array([[175,181,159,165]]).T\n",
    "\n",
    "#finding the left inverse. both of these methods return the same\n",
    "X_leftinv1=np.linalg.inv(X.T@X)@X.T\n",
    "X_leftinv=np.linalg.pinv(X)\n",
    "\n",
    "B=X_leftinv@y\n",
    "\n",
    "print(f'the soltuion to XB=y is:\\n{B}\\n')\n",
    "\n",
    "print(f'Rank of X: {np.linalg.matrix_rank(X)}')\n",
    "\n",
    "#finding if y is in the column space of X\n",
    "Xy= np.hstack((X,y))\n",
    "print(f'Rank of [X|y]: {np.linalg.matrix_rank(Xy)}')\n",
    "\n",
    "e=X@B-y\n",
    "print(f'The residual is XB-y (this is not zero if the two ranks above not equal):\\n{np.round(e,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3fe8f-3a41-47aa-a5c2-20b92c6eb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "numcourses = [13,4,12,3,14,13,12,9,11,7,13,11,9,2,5,7,10,0,9,7]\n",
    "happiness  = [70,25,54,21,80,68,84,62,57,40,60,64,45,38,51,52,58,21,75,70]\n",
    "\n",
    "\n",
    "# Build a statistical model with an intercept\n",
    "\n",
    "# design matrix as a column vector\n",
    "X = np.hstack((np.ones((20,1)),np.array(numcourses,ndmin=2).T))\n",
    "print(X.shape)\n",
    "\n",
    "# fit the model using the left-inverse\n",
    "X_leftinv = np.linalg.inv(X.T@X) @ X.T\n",
    "\n",
    "# solve for the coefficients\n",
    "beta = X_leftinv @ happiness\n",
    "beta\n",
    "\n",
    "# let's plot it!\n",
    "\n",
    "# predicted data\n",
    "pred_happiness = X@beta\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "\n",
    "# plot the data and predicted values\n",
    "plt.plot(numcourses,happiness,'ks',markersize=15)\n",
    "plt.plot(numcourses,pred_happiness,'o-',color=[.6,.6,.6],linewidth=3,markersize=8)\n",
    "\n",
    "# plot the residuals (errors)\n",
    "for n,y,yHat in zip(numcourses,happiness,pred_happiness):\n",
    "  plt.plot([n,n],[y,yHat],'--',color=[.8,.8,.8],zorder=-10)\n",
    "\n",
    "plt.xlabel('Number of courses taken')\n",
    "plt.ylabel('General life happiness')\n",
    "plt.xlim([-1,15])\n",
    "plt.ylim([0,100])\n",
    "plt.xticks(range(0,15,2))\n",
    "plt.legend(['Real data','Predicted data','Residual'],fontsize=5)\n",
    "plt.title(f'SSE = {np.sum((pred_happiness-happiness)**2):.2f}')\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f27aef-c8f6-48e7-a71d-83ac2a8af4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exer 1 and 2\n",
    "\n",
    "# compute residual\n",
    "res = happiness-pred_happiness\n",
    "\n",
    "\n",
    "# should be zero + some error\n",
    "print('Dot product: ' + str(np.round(np.dot(pred_happiness,res),2)))\n",
    "print('Correlation: ' + str(np.round(np.corrcoef(pred_happiness,res)[0,1],2)))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "# show in a plot\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(res,pred_happiness,'ko',markersize=12)\n",
    "plt.xlabel('Residual error')\n",
    "plt.ylabel('Model-predicted values')\n",
    "plt.title(f'r = {np.corrcoef(pred_happiness,res)[0,1]:.20f}')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# the residual is orthogonal to the entire column space of the design matrix.\n",
    "\n",
    "# I demonstrated this by showing that the residuals vector is in the left-null space of the design matrix.\n",
    "# I did that by using scipy.linalg.null_space to find the left-null space, augmenting that null-space basis\n",
    "# matrix by the residuals vector, and showing that the null space and augmented null space have the same rank.\n",
    "\n",
    "\n",
    "# compute the null space (via scipy.linalg)\n",
    "nullspace = scipy.linalg.null_space(X.T)\n",
    "\n",
    "\n",
    "# augment the residuals\n",
    "nullspaceAugment = np.hstack((nullspace,res.reshape(-1,1)) )\n",
    "\n",
    "\n",
    "# print their ranks\n",
    "print(f'dim(  N(X)    ) = {np.linalg.matrix_rank(nullspace)}')\n",
    "print(f'dim( [N(X)|r] ) = {np.linalg.matrix_rank(nullspaceAugment)}')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd239d5-3f9d-487d-a473-5b12d3ef82b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=20\n",
    "N=3\n",
    "\n",
    "X=np.random.randn(M,N)*3\n",
    "y=np.random.randn(M,1)*5\n",
    "\n",
    "#method 1: Left-Inverse\n",
    "X_inv1=np.linalg.pinv(X)\n",
    "X_inv2=np.linalg.pinv(X.T@X)@X.T\n",
    "\n",
    "if(np.max(abs(X_inv1-X_inv2)))<0.00001:\n",
    "    print(f'The two inverse methods return same matrix inverse')\n",
    "else:\n",
    "    print(f'ERRRROOOOORRR')\n",
    "\n",
    "B_1 = X_inv1@y\n",
    "\n",
    "#method 2: QR\n",
    "\n",
    "Q,R=np.linalg.qr(X,\"reduced\")\n",
    "\n",
    "print(f'Q: {Q.shape}, R: {R.shape}')\n",
    "\n",
    "B_2 = np.linalg.pinv(R)@Q.T@y\n",
    "\n",
    "\n",
    "#Method 3: RREF\n",
    "Xy = np.hstack((R,Q.T@y))\n",
    "\n",
    "\n",
    "symMat = sym.Matrix(Xy)\n",
    "RREF = symMat.rref()[0]\n",
    "\n",
    "RREF=np.array(RREF)\n",
    "B_3=np.array([RREF[:,-1]]).astype(float)\n",
    "\n",
    "print(f'Betas from Left-Inv:\\n {np.round(B_1,3).T}')\n",
    "print(f'Betas from QRE:\\n {np.round(B_2,3).T}')\n",
    "print(f'Betas from RREF:\\n {np.round(B_3.T,3).T}\\n')\n",
    "\n",
    "print(f'R:\\n{np.round(R,3)}')\n",
    "print(f'R|QTy\\n{np.round(Xy,3)}')\n",
    "print(f'RREF(R|QTy)\\n{np.round(RREF.astype(float),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076a405-74d0-46ec-942d-18b69c3ba458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exer 5\n",
    "n=50\n",
    "X=np.random.randn(n,n)\n",
    "Y=np.eye(n)\n",
    "Xinv_1=np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    Xinv_1[:,i]=(np.linalg.pinv(X)@Y[:,i])\n",
    "    \n",
    "Xinv_2 = np.linalg.inv(X.T@X) @ X.T @ Y\n",
    "\n",
    "Xinv_3 = np.linalg.inv(X)\n",
    "\n",
    "print(np.round(np.sum(abs(Xinv_1@X-np.eye(n))),15))\n",
    "print(np.round(np.sum(abs(Xinv_2@X-np.eye(n))),15))\n",
    "print(np.round(np.sum(abs(Xinv_3@X-np.eye(n))),15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f140e8-8239-4385-8340-13f99aad4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.linspace(0,3,12)\n",
    "\n",
    "print(A)\n",
    "print(A.shape)\n",
    "print(\"\\n\")\n",
    "A_r=A.reshape(3,-1)\n",
    "print(A_r)\n",
    "print(A_r.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
